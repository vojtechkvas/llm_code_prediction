{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_to_plot = ['chrf', 'rouge', 'ter', 'bleu', 'codebleu', \"manual_evaluation\"]\n",
    "metrics_to_plot_label = ['chrf', 'rouge', '1 - min(ter , 1)', 'bleu', 'codebleu', \"manual_evaluation\"]\n",
    "\n",
    "file_names_to_exclude = [\"data/member/repository/MemberEntityListener.java\",\n",
    "                          \"data/member/controller/RoomController.java\", \n",
    "                          \"data/member/business/RoomAccommodationService.java\", \n",
    "                          \"data/member/business/LanguageService.java\", \n",
    "                          \"data/member/business/AssociateFacultyService.java\", \n",
    "                          \"data/member/business/FacultyService.java\", \n",
    "                          \"data/member/business/BlockAccessService.java\", \n",
    "                          \"data/member/dto/RoleAssignmentCreateDto.java\", \n",
    "                          ]\n",
    "\n",
    "dataset_path = \"data/preprocess_and_result_data/data_manually_evaluated.json\" \n",
    "with open(dataset_path, 'r') as f:\n",
    "    metric_dataset = json.load(f)\n",
    " \n",
    "\"\"\"\n",
    "# Filter out the files that are in the exclude list\n",
    "metric_dataset = [\n",
    "    item for item in metric_dataset \n",
    "    if item.get(\"filename\") not in file_names_to_exclude\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract data for plotting\n",
    "file_names = [item['filename'] for item in metric_dataset]\n",
    "num_files = len(file_names)\n",
    "num_metrics = len(metrics_to_plot)\n",
    "bar_width = 0.8 / num_metrics\n",
    "index = range(num_files)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))  # Adjust figure size as needed\n",
    "\n",
    "\n",
    "for i, metric_name in enumerate(metrics_to_plot):\n",
    "    metric_values = []\n",
    "    for item in metric_dataset:\n",
    "        score = 0\n",
    "        if metric_name == 'chrf':\n",
    "            score = item.get(metric_name, {}).get('score', 0) / 100.0\n",
    "            metric_values.append(score)\n",
    "        elif metric_name == 'rouge':\n",
    "            score = item.get(metric_name, {}).get('avg', 1)\n",
    "         \n",
    "            metric_values.append(score)\n",
    "        elif metric_name == 'ter':\n",
    "            score =  1- min(  item.get(metric_name, {}).get('score', 0) / 100.0, 1) \n",
    "          #  score =    item.get(metric_name, {}).get('score', 0) / 100.0\n",
    "            metric_values.append(score)\n",
    "          #   display(\"ter \" + str(score) + \" \" + str(item.get(metric_name, {}).get('score', 0)))\n",
    "        elif metric_name == 'bleu' or metric_name == 'codebleu':\n",
    "            score = item.get(metric_name, {}).get(metric_name, 0)  # Assuming 'bleu' is directly under the top level\n",
    "            metric_values.append(score)\n",
    "        elif metric_name == 'manual_evaluation':\n",
    "\n",
    "            score = item.get(metric_name, 0)  \n",
    "        \n",
    "            if score != None  :\n",
    "                score = score / 10.0\n",
    "            else:\n",
    "                score = 0\n",
    "            \n",
    "            metric_values.append(score)\n",
    "        else:\n",
    "            score = item.get(metric_name, 0)  # Default to 0 if metric not found\n",
    "            metric_values.append(score)\n",
    "\n",
    "    positions = [p + i * bar_width for p in index]\n",
    "    ax.bar(positions, metric_values, bar_width, label=metrics_to_plot_label[i], alpha=0.7)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Files\")\n",
    "ax.set_ylabel(\"Metric Score\")\n",
    "ax.set_title(\"Metric Comparison Across Files\")\n",
    "ax.set_xticks([p + bar_width * (num_metrics - 1) / 2 for p in index])\n",
    "\n",
    "x_labels = [item['filename'].split(\"/\")[-1] for item in metric_dataset]\n",
    "\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_xlim(-0.5, num_files ) \n",
    "ax.set_xlim(-0.5, num_files+ 0.5 ) \n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    " # Adjust to match the number of files\n",
    "\n",
    "\n",
    "\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dataset = []\n",
    "\n",
    "for item in metric_dataset:\n",
    "    row = {}\n",
    "   #  for metric_name in metrics_to_plot:\n",
    "    for i, metric_name in enumerate(metrics_to_plot):\n",
    "        if metric_name == 'chrf':\n",
    "            score = item.get(metric_name, {}).get('score', 0)  \n",
    "            row[metrics_to_plot[i]] = score\n",
    "        elif metric_name == 'rouge':\n",
    "            score_avg = item.get(metric_name, {}).get('avg', 1)\n",
    "            score_rouge1 = item.get(metric_name, {}).get('rouge1', 1)\n",
    "            score_rouge2 = item.get(metric_name, {}).get('rouge2', 1)\n",
    "            score_rougeL = item.get(metric_name, {}).get('rougeL', 1)\n",
    "            score_rougeLsum = item.get(metric_name, {}).get('rougeLsum', 1)\n",
    "\n",
    "            row[metrics_to_plot[i] + \" avg\"] = score_avg\n",
    "            row[metrics_to_plot[i] + \" rouge1\"] = score_rouge1\n",
    "            row[metrics_to_plot[i] + \" rouge2\"] = score_rouge2\n",
    "            row[metrics_to_plot[i] + \" rougeL\"] = score_rougeL\n",
    "            row[metrics_to_plot[i] + \" rougeLsum\"] = score_rougeLsum\n",
    "\n",
    "        elif metric_name == 'ter':\n",
    "            score =  1- min(  item.get(metric_name, {}).get('score', 0) / 100.0, 1) \n",
    "            score = item.get(metric_name, {}).get('score', 0) \n",
    "            row[metrics_to_plot[i]] = score\n",
    "        elif metric_name == 'bleu' or metric_name == 'codebleu':\n",
    "            score = item.get(metric_name, {}).get(metric_name, 0)\n",
    "            row[metrics_to_plot[i]] = score\n",
    "        elif metric_name == 'manual_evaluation':\n",
    "            score = item.get(metric_name, 0)\n",
    "            if score is not None:\n",
    "                score = score / 10.0\n",
    "            else:\n",
    "                score = 0\n",
    "            row[metrics_to_plot[i]] = score\n",
    "        else:\n",
    "            row[metrics_to_plot[i]] = item.get(metric_name, 0)\n",
    "\n",
    "    matrix_dataset.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dataset = pd.DataFrame(matrix_dataset)\n",
    "\n",
    "def display_correlation(matrix_dataset, method='pearson'):\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(matrix_dataset)\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = df.corr(method=method)  # Options: 'pearson', 'spearman', 'kendall'\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Metric Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "correlation_matrix = display_correlation(matrix_dataset)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
